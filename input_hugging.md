---
title: HuggingFace Transformer
date: 2023-02-26 23:10:08
categories:
- Language
tags: [GPT, LLM, Hugging Face]
typora-root-url: ../../allenlu2009.github.io


---



## Reference

HuggingFace

[@liHuggingfaceTransformer2021]: very good and clear introduction


## Introduction

Transformersçš„ç›®çš„æ˜¯ç‚ºäº†ï¼š

- å¹«åŠ©NLPç ”ç©¶è€…é€²è¡Œå¤§è¦æ¨¡çš„transformeræ¨¡å‹

- å¹«åŠ©å·¥æ¥­ç•Œçš„ä½¿ç”¨è€…å¾®èª¿æ¨¡å‹ä¸¦ä¸”ä¸æ˜¯åˆ°ç”Ÿç”¢ç’°å¢ƒ
- å¹«åŠ©å·¥ç¨‹å¸«ä¸‹è¼‰é è¨“ç·´æ¨¡å‹ä¸¦ä¸”è§£æ±ºå¯¦éš›å•é¡Œ

å®ƒçš„è¨­è¨ˆåŸå‰‡åŒ…æ‹¬ï¼š

* æ˜“ç”¨
  * åªæœ‰configurationï¼Œmodelså’Œtokenizerä¸‰å€‹ä¸»è¦é¡ã€‚
  * æ‰€æœ‰çš„æ¨¡å‹éƒ½å¯ä»¥é€šéçµ±ä¸€çš„from_pretrained()å‡½æ•¸ä¾†å¯¦ç¾åŠ è¼‰ï¼Œtransformersæœƒè™•ç†ä¸‹è¼‰ã€ç·©å­˜å’Œå…¶å®ƒæ‰€æœ‰åŠ è¼‰æ¨¡å‹ç›¸é—œçš„ç´°ç¯€ã€‚è€Œæ‰€æœ‰é€™äº›æ¨¡å‹éƒ½çµ±ä¸€åœ¨Hugging Face Modelsç®¡ç†ã€‚
  * åŸºäºä¸Šé¢çš„ä¸‰å€‹é¡ï¼Œæä¾›æ›´ä¸Šå±¤çš„pipelineå’ŒTrainer/TFTrainerï¼Œå¾è€Œç”¨æ›´å°‘çš„ä»£ç¢¼å¯¦ç¾æ¨¡å‹çš„é æ¸¬å’Œå¾®èª¿ã€‚
  * å› æ­¤å®ƒä¸æ˜¯ä¸€å€‹åŸºç¤çš„ç¥ç¶“ç¶²çµ¡åº«ä¾†ä¸€æ­¥ä¸€æ­¥æ§‹é€ Transformerï¼Œè€Œæ˜¯æŠŠå¸¸è¦‹çš„Transformeræ¨¡å‹å°è£æˆä¸€å€‹building blockï¼Œæˆ‘å€‘å¯ä»¥æ–¹ä¾¿çš„åœ¨PyTorchæˆ–è€…TensorFlowè£¡ä½¿ç”¨å®ƒã€‚
* å„˜é‡å’ŒåŸè«–æ–‡ä½œè€…çš„å¯¦ç¾ä¸€è‡´
  * **æ¯å€‹æ¨¡å‹è‡³å°‘æœ‰ä¸€å€‹ä¾‹å­å¯¦ç¾å’ŒåŸè«–æ–‡é¡ä¼¼çš„æ•ˆæœ**
  * å„˜é‡åƒè€ƒåŸè«–æ–‡çš„å¯¦ç¾ï¼Œå› æ­¤æœ‰äº›ä»£ç¢¼ä¸æœƒé‚£éº¼è‡ªç„¶



### ä¸»è¦æ¦‚å¿µ

* è«¸å¦‚BertModelçš„æ¨¡å‹(Model)é¡ï¼ŒåŒ…æ‹¬30+PyTorchæ¨¡å‹(torch.nn.Module)å’Œå°æ‡‰çš„TensorFlowæ¨¡å‹(tf.keras.Model)ã€‚
* è«¸å¦‚BertConfigçš„é…ç½®(Config)é¡ï¼Œå®ƒä¿å­˜äº†æ¨¡å‹çš„ç›¸é—œ(è¶…)åƒæ•¸ã€‚æˆ‘å€‘é€šå¸¸ä¸éœ€è¦è‡ªå·±ä¾†æ§‹é€ å®ƒã€‚å¦‚æœæˆ‘å€‘ä¸éœ€è¦é€²è¡Œæ¨¡å‹çš„ä¿®æ”¹ï¼Œé‚£éº¼å‰µå»ºæ¨¡å‹æ™‚æœƒè‡ªå‹•ä½¿ç”¨å°æ–¼çš„é…ç½®
* è«¸å¦‚BertTokenizerçš„Tokenizeré¡ï¼Œå®ƒä¿å­˜äº†è©å…¸ç­‰ä¿¡æ¯ä¸¦ä¸”å¯¦ç¾äº†æŠŠå­—å…ƒä¸²è®ŠæˆIDåºåˆ—çš„åŠŸèƒ½ã€‚

æ‰€æœ‰é€™ä¸‰é¡å°è±¡éƒ½å¯ä»¥ä½¿ç”¨from_pretrained()å‡½æ•¸è‡ªå‹•é€šéåå­—æˆ–è€…ç›®éŒ„é€²è¡Œæ§‹é€ ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨save_pretrained()å‡½æ•¸ä¿å­˜ã€‚

### 

### ä½¿ç”¨pipeline

ä½¿ç”¨é è¨“ç·´æ¨¡å‹æœ€ç°¡å–®çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨pipelineå‡½æ•¸ï¼Œå®ƒæ”¯æŒå¦‚ä¸‹çš„ä»»å‹™ï¼š

- æƒ…æ„Ÿåˆ†æ(Sentiment analysis)ï¼šä¸€æ®µæ–‡æœ¬æ˜¯æ­£é¢é‚„æ˜¯è² é¢çš„æƒ…æ„Ÿå‚¾å‘
- æ–‡æœ¬ç”Ÿæˆ(Text generation)ï¼šçµ¦å®šä¸€æ®µæ–‡æœ¬ï¼Œè®“æ¨¡å‹è£œå……å¾Œé¢çš„å…§å®¹
- å‘½åå¯¦é«”è­˜åˆ¥(Name entity recognition)ï¼šè­˜åˆ¥æ–‡å­—ä¸­å‡ºç¾çš„äººååœ°åçš„å‘½åå¯¦é«”
- å•ç­”(Question answering)ï¼šçµ¦å®šä¸€æ®µæ–‡æœ¬ä»¥åŠé‡å°å®ƒçš„ä¸€å€‹å•é¡Œï¼Œå¾æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆ
- å¡«è©(Filling masked text)ï¼šæŠŠä¸€æ®µæ–‡å­—çš„æŸäº›éƒ¨åˆ†maskä½ï¼Œç„¶å¾Œè®“æ¨¡å‹å¡«ç©º
- æ‘˜è¦(Summarization)ï¼šæ ¹æ“šä¸€æ®µé•·æ–‡æœ¬ä¸­ç”Ÿæˆç°¡çŸ­çš„æ‘˜è¦
- ç¿»è­¯(Translation)ï¼šæŠŠä¸€ç¨®èªè¨€çš„æ–‡å­—ç¿»è­¯æˆå¦ä¸€ç¨®èªè¨€
- ç‰¹å¾µæå–(Feature extraction)ï¼šæŠŠä¸€æ®µæ–‡å­—ç”¨ä¸€å€‹å‘é‡ä¾†è¡¨ç¤º



## Foundation Network

åŸºæœ¬æ‰€æœ‰çš„ LLM éƒ½æ˜¯ç”¨ transformer network ä½œç‚º foundation network.

å·®åˆ¥åªæ˜¯å¤§å°ï¼Œencoder/decoder/bothï¼Œtraining çš„æ–¹å¼ã€‚



<img src="/media/image-20230301224435617.png" alt="image-20230301224435617" style="zoom: 67%;" />

Hugging Face å°æ–¼ NLP æä¾›å¾ˆå¥½çš„ tutorialã€‚ä»¥ä¸‹æ˜¯ NLP language model (ä¸å« image)  çš„åˆ†é¡ï¼ŒåŒ…å« tasks. 

| Model           | Examples                                   | Tasks                                                        |
| --------------- | ------------------------------------------ | ------------------------------------------------------------ |
| Encoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Sentence classification, named entity recognition, extractive question answering |
| Decoder         | CTRL, GPT, GPT-2, Transformer XL           | Text generation                                              |
| Encoder-decoder | BART, T5, Marian, mBART                    | Summarization, translation, generative question answering    |

HunggingFace çš„ tutorial å¾ˆå¥½çš„èª¬æ˜é€™ä¸€é»ï¼š

Transformer class å®šç¾© pipeline module.

pipeline çš„åƒæ•¸åŒ…å«

| Downstream Task          | Type                                                  | Default Network                                 |
| ------------------------ | ----------------------------------------------------- | ----------------------------------------------- |
| sentiment-analysis       | (bi-direction) encoder                                | distilbert-base-uncased-finetuned-sst-2-english |
| zero-shot-classification | (bi-direction) encoder+ (auto-regression) decoder (?) | bart-large-mnli                                 |
| text-generation          | (auto-regression) decoder                             | gpt2 or dsitilgpt2                              |
| full-mask å¡«ç©º           | (bi-direction) encoder                                | distilroberta                                   |
| question-answering       | (bi-direction) encoder                                | distilbert-base-cased-distilled-squad           |
| summarization            | (bi-direction) encoder+ (auto-regression) decoder     | distilbart-cnn-12-6                             |
| translation              | opus-mt-fr-en                                         | opus-mt-fr-en                                   |



* sentiment-analysis:  default ä½¿ç”¨ distilled bert:  distilbert-base-uncased-finetuned-sst-2-english

ä¸‹é¢æˆ‘å€‘ä¾†çœ‹ä¸€å€‹æƒ…æ„Ÿåˆ†æçš„ä¾‹å­ï¼š

```python
from transformers import pipeline
classifier = pipeline('sentiment-analysis')
classifier("I've been waiting for a HuggingFace course my whole life.")
```

ç•¶ç¬¬ä¸€æ¬¡é‹è¡Œçš„æ™‚å€™ï¼Œå®ƒæœƒä¸‹è¼‰é è¨“ç·´æ¨¡å‹ (é€™å€‹ pipeline task é è¨­æ˜¯ distilbert-base-uncased-finetuned-sst-2-english, pytorch_model.bin, 268MB) å’Œåˆ†è©å™¨ (tokenizer, tokenizer_config.json) ä¸¦ä¸”ç·©å­˜ä¸‹ä¾†ã€‚åˆ†è©å™¨çš„å·¦å³æ˜¯æŠŠæ–‡æœ¬è™•ç†æˆæ•´æ•¸åºåˆ—ã€‚æœ€çµ‚é‹è¡Œçš„çµæœç‚ºï¼š

```python
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```



æˆ‘å€‘ä¹Ÿå¯ä»¥ä¸€æ¬¡é æ¸¬å¤šå€‹çµæœï¼š

```python
results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.",
           "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
```

é‹è¡Œçµæœç‚ºï¼š

```python
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

ä¸Šé¢çš„ç¬¬äºŒå€‹å¥å­è¢«åˆ†æˆäº†è² é¢çš„æƒ…æ„Ÿï¼Œé€™æ˜¯å› ç‚ºæ¨¡å‹æ˜¯äºŒåˆ†é¡çš„æ¨¡å‹ï¼Œä¸éå®ƒçš„å¾—åˆ†æ˜¯åœ¨0~1ä¹‹é–“æ¯”è¼ƒå±…ä¸­çš„éƒ¨åˆ†ã€‚é è¨­çš„ â€sentiment-analysisâ€ æœƒä½¿ç”¨ distilbert-base-uncased-finetuned-sst-2-english æ¨¡å‹ã€‚å®ƒæ˜¯æŠŠ DistilBERT æ¨¡å‹åœ¨ SST-2 é€™å€‹ä»»å‹™ä¸Š fine-tuning å¾Œçš„çµæœã€‚

æˆ‘å€‘ä¹Ÿå¯ä»¥æŒ‡å®šå…¶å®ƒçš„æƒ…æ„Ÿåˆ†é¡æ¨¡å‹ï¼Œæ¯”å¦‚æˆ‘å€‘å¯èƒ½éœ€è¦å°æ³•èªé€²è¡Œæƒ…æ„Ÿåˆ†é¡ï¼Œé‚£éº¼ä¸Šé¢çš„æ¨¡å‹æ˜¯ä¸é©åˆçš„ï¼Œæˆ‘å€‘å¯ä»¥å»model hubå°‹æ‰¾åˆé©çš„æ¨¡å‹ã€‚æ¯”å¦‚æˆ‘å€‘å¯ä»¥ä½¿ç”¨ï¼š

```python
classifier = pipeline('sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")
```

é™¤äº†é€šéåå­—ä¾†åˆ¶å®šmodelåƒæ•¸ï¼Œæˆ‘å€‘ä¹Ÿå¯ä»¥å‚³çµ¦modelä¸€å€‹åŒ…å«æ¨¡å‹çš„ç›®éŒ„çš„è·¯å¾‘ï¼Œä¹Ÿå¯ä»¥å‚³éä¸€å€‹æ¨¡å‹ objectã€‚å¦‚æœæˆ‘å€‘æƒ³å‚³éæ¨¡å‹objectï¼Œé‚£éº¼ä¹Ÿéœ€è¦å‚³å…¥tokenizerã€‚

æˆ‘å€‘éœ€è¦å…©å€‹ classï¼Œä¸€å€‹æ˜¯ AutoTokenizerï¼Œæˆ‘å€‘å°‡ä½¿ç”¨å®ƒä¾†ä¸‹è¼‰å’ŒåŠ è¼‰èˆ‡æ¨¡å‹åŒ¹é…çš„Tokenizerã€‚å¦ä¸€å€‹æ˜¯AutoModelForSequenceClassificationã€‚æ³¨æ„ï¼šæ¨¡å‹é¡æ˜¯èˆ‡ä»»å‹™ç›¸é—œçš„ï¼Œæˆ‘å€‘é€™è£¡æ˜¯æƒ…æ„Ÿåˆ†é¡çš„åˆ†é¡ä»»å‹™ï¼Œæ‰€ä»¥æ˜¯AutoModelForSequenceClassificationã€‚

æˆ‘å€‘åœ¨ä½¿ç”¨å‰éœ€è¦importï¼š

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
```

ç‚ºäº†ä¸‹è¼‰å’ŒåŠ è¼‰æ¨¡å‹ï¼Œæˆ‘å€‘åªéœ€è¦ä½¿ç”¨ from_pretrained å‡½æ•¸ï¼š

```python
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
```



### Pipeline åŸç† = Tokenizer + Model

ä¸‹é¢æˆ‘å€‘ä¾†çœ‹ä¸€ä¸‹pipelineå¯¦éš›åšäº†å“ªäº›å·¥ä½œã€‚é¦–å…ˆå®ƒæœƒä½¿ç”¨å‰é¢çš„from_pretrainedå‡½æ•¸**åŠ è¼‰Tokenizerå’Œæ¨¡å‹**ï¼š

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```



### ä½¿ç”¨ Tokenizer

**Tokenizerçš„ä½œç”¨å¤§è‡´å°±æ˜¯åˆ†è©ï¼Œç„¶å¾ŒæŠŠè©è®Šæˆçš„æ•´æ•¸ID**ï¼Œç•¶ç„¶æœ‰äº›æ¨¡å‹æœƒä½¿ç”¨ subwordã€‚ä½†æ˜¯ä¸ç®¡æ€éº¼æ¨£ï¼Œæœ€çµ‚çš„ç›®çš„æ˜¯æŠŠä¸€æ®µæ–‡æœ¬è®ŠæˆIDçš„åºåˆ—ã€‚ç•¶ç„¶å®ƒä¹Ÿå¿…é ˆèƒ½å¤ åéä¾†æŠŠIDåºåˆ—è®Šæˆæ–‡æœ¬ã€‚é—œæ–¼Tokenizeræ›´è©³ç´°çš„ä»‹ç´¹è«‹åƒè€ƒ[é€™è£¡](https://huggingface.co/docs/transformers/tokenizer_summary)ï¼Œå¾Œé¢æˆ‘å€‘ä¹Ÿæœƒæœ‰å°æ‡‰çš„è©³ç´°ä»‹ç´¹ã€‚

ä¸‹é¢æˆ‘å€‘ä¾†çœ‹ä¸€å€‹ä¾‹å­ï¼š

```python
inputs = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
```

Tokenizer object æ˜¯ callableï¼Œå› æ­¤å¯ä»¥ç›´æ¥å‚³å…¥ä¸€å€‹å­—å…ƒä¸²ï¼Œè¿”å›ä¸€å€‹ dictã€‚æœ€ä¸»è¦çš„æ˜¯IDçš„listï¼ŒåŒæ™‚ä¹Ÿæœƒè¿”å›attention maskï¼š

```python
print(inputs)
{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

æˆ‘å€‘ä¹Ÿå¯ä»¥ä¸€æ¬¡å‚³å…¥ä¸€å€‹batchçš„å­—å…ƒä¸²ï¼Œé€™æ¨£ä¾¿äºæ‰¹é‡è™•ç†ã€‚é€™æ™‚æˆ‘å€‘éœ€è¦æŒ‡å®špaddingç‚ºTrueä¸¦ä¸”è¨­ç½®æœ€å¤§çš„é•·åº¦ï¼š

```python
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt"
)
```

truncationç‚ºTrueæœƒæŠŠéé•·çš„è¼¸å…¥åˆ‡æ‰ï¼Œå¾è€Œä¿è­‰æ‰€æœ‰çš„å¥å­éƒ½æ˜¯ç›¸åŒé•·åº¦çš„ï¼Œreturn_tensors=â€ptâ€è¡¨ç¤ºè¿”å›çš„æ˜¯PyTorchçš„Tensorï¼Œå¦‚æœä½¿ç”¨TensorFlowå‰‡éœ€è¦è¨­ç½®return_tensors=â€tfâ€ã€‚

æˆ‘å€‘å¯ä»¥æŸ¥çœ‹åˆ†è©çš„çµæœï¼š

```
>>> for key, value in pt_batch.items():
...     print(f"{key}: {value.numpy().tolist()}")
input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]
attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]
```

pt_batchä»ç„¶æ˜¯ä¸€å€‹dictï¼Œinput_idsæ˜¯ä¸€å€‹batchçš„IDåºåˆ—ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°ç¬¬äºŒå€‹å­—å…ƒä¸²è¼ƒçŸ­ï¼Œæ‰€ä»¥å®ƒè¢«paddingæˆå’Œç¬¬ä¸€å€‹ä¸€æ¨£é•·ã€‚å¦‚æœæŸå€‹å¥å­çš„é•·åº¦è¶…émax_lengthï¼Œä¹Ÿæœƒè¢«åˆ‡æ‰å¤šé¤˜çš„éƒ¨åˆ†ã€‚



### ä½¿ç”¨æ¨¡å‹

Tokenizerçš„è™•ç†çµæœå¯ä»¥è¼¸å…¥çµ¦æ¨¡å‹ï¼Œå°æ–¼TensorFlowä¾†èªªç›´æ¥è¼¸å…¥å°±è¡Œï¼Œè€Œå°æ–¼PyTorchå‰‡éœ€è¦ä½¿ç”¨ \*\* ä¾†å±•é–‹åƒæ•¸ï¼š

```python
# PyTorch
pt_outputs = pt_model(**pt_batch)
# TensorFlow
tf_outputs = tf_model(tf_batch)
```

Transformersçš„æ‰€æœ‰è¼¸å‡ºéƒ½æ˜¯tupleï¼Œå³ä½¿åªæœ‰ä¸€å€‹çµæœä¹Ÿæœƒæ˜¯é•·åº¦ç‚º1çš„tupleï¼š

```
print(pt_outputs)
(tensor([[-4.0833,  4.3364],
[ 0.0818, -0.0418]], grad_fn=<AddmmBackward>),)
```
Transformersçš„æ¨¡å‹é è¨­è¿”å›logitsï¼Œå¦‚æœéœ€è¦æ¦‚ç‡ï¼Œå¯ä»¥è‡ªå·±åŠ softmaxï¼š

```
import torch.nn.functional as F
pt_predictions = F.softmax(pt_outputs[0], dim=-1)
```
å¾—åˆ°å’Œå‰é¢ä¸€æ¨£çš„çµæœï¼š

```
print(pt_predictions)
tensor([[2.2043e-04, 9.9978e-01],
[5.3086e-01, 4.6914e-01]], grad_fn=<SoftmaxBackward>)
```
å¦‚æœæˆ‘å€‘æœ‰è¼¸å‡ºåˆ†é¡å°æ‡‰çš„æ¨™ç±¤ï¼Œé‚£éº¼ä¹Ÿå¯ä»¥å‚³å…¥ï¼Œé€™æ¨£å®ƒé™¤äº†æœƒè¨ˆç®—logitsé‚„æœƒlossï¼š

```
import torch
pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))
```
è¼¸å‡ºç‚ºï¼š

```
SequenceClassifierOutput(loss=tensor(0.3167, grad_fn=<NllLossBackward>), logits=tensor([[-4.0833,  4.3364],
        [ 0.0818, -0.0418]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)
```

**from_pretrained è¿”å›çš„æ˜¯ PyTorch çš„ torch.nn.Module æˆ–è€… TensorFlow çš„ tf.keras.Modelã€‚å› æ­¤æˆ‘å€‘å¯ä»¥å¾ˆå®¹æ˜“çš„æŠŠ Transformerèå…¥æˆ‘å€‘çš„ä»£ç¢¼è£¡ï¼Œè‡ªå·±ä¾†å¯¦ç¾è¨“ç·´æˆ–è€…é æ¸¬ã€‚ä½†æ˜¯TransformersåŒ…å…§ç½®äº†ä¸€å€‹Trainer classï¼Œæ–¹ä¾¿æˆ‘å€‘è¨“ç·´æˆ–è€…fine-tuningã€‚**

æˆ‘å€‘è¨“ç·´å®Œæˆå¾Œå°±å¯ä»¥ä¿å­˜æ¨¡å‹åˆ°ä¸€å€‹ç›®éŒ„ä¸­ï¼š

```
tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)
```

ä¹‹å¾Œæˆ‘å€‘æƒ³ç”¨çš„æ™‚å€™éš¨æ™‚å¯ä»¥ä½¿ç”¨from_pretrainedå‡½æ•¸åŠ è¼‰å®ƒå€‘ã€‚TransformersåŒ…éå¸¸é…·çš„ä¸€é»å°±æ˜¯å®ƒå¯ä»¥è¼•é¬†çš„åœ¨PyTorchå’ŒTensorFlowä¹‹é–“åˆ‡æ›ã€‚æ¯”å¦‚ä¸‹é¢çš„ä¾‹å­æ˜¯ä¿å­˜PyTorchçš„æ¨¡å‹ç„¶å¾Œç”¨TensorFlowåŠ è¼‰ï¼š

```
tokenizer = AutoTokenizer.from_pretrained(save_directory)
model = TFAutoModel.from_pretrained(save_directory, from_pt=True)
```

å¦‚æœç”¨PyTorchåŠ è¼‰TensorFlowæ¨¡å‹ï¼Œå‰‡éœ€è¦è¨­ç½®from_tf=Trueï¼š

```
tokenizer = AutoTokenizer.from_pretrained(save_directory)
model = AutoModel.from_pretrained(save_directory, from_tf=True)
```

é™¤äº†logitsï¼Œæˆ‘å€‘ä¹Ÿå¯ä»¥è¿”å›æ‰€æœ‰çš„éš±ç‹€æ…‹å’Œattentionï¼š

```
pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)
all_hidden_states, all_attentions = pt_outputs[-2:]
```

å…·é«”çš„æ¨¡å‹é¡
AutoModelå’ŒAutoTokenizeråªæ˜¯æ–¹ä¾¿æˆ‘å€‘ä½¿ç”¨ï¼Œä½†æœ€çµ‚æœƒæ ¹æ“šä¸åŒçš„æ¨¡å‹(åå­—æˆ–è€…è·¯å¾‘)æ§‹é€ ä¸åŒçš„æ¨¡å‹å°è±¡ä»¥åŠèˆ‡ä¹‹åŒ¹é…çš„Tokenizerã€‚å‰é¢çš„ä¾‹å­æˆ‘å€‘ä½¿ç”¨çš„æ˜¯distilbert-base-uncased-finetuned-sst-2-englishé€™å€‹åå­—ï¼ŒAutoModelForSequenceClassification æœƒè‡ªå‹•çš„å¹«æˆ‘å€‘åŠ è¼‰DistilBertForSequenceClassificationæ¨¡å‹ã€‚

çŸ¥é“åŸç†å¾Œæˆ‘å€‘ä¹Ÿå¯ä»¥è‡ªå·±ç›´æ¥æ§‹é€ é€™äº›æ¨¡å‹ï¼š

```
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = DistilBertForSequenceClassification.from_pretrained(model_name)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```

è‡ªå®šç¾©æ¨¡å‹
å¦‚æœä½ æƒ³è‡ªå®šç¾©æ¨¡å‹(é€™è£¡æŒ‡çš„æ˜¯èª¿æ•´æ¨¡å‹çš„è¶…åƒæ•¸ï¼Œæ¯”å¦‚ç¶²çµ¡çš„å±¤æ•¸ï¼Œæ¯å±¤çš„attention headå€‹æ•¸ç­‰ç­‰ï¼Œå¦‚æœä½ è¦å¯¦ç¾ä¸€å€‹å…¨æ–°çš„æ¨¡å‹ï¼Œé‚£å°±ä¸èƒ½ç”¨é€™è£¡çš„æ–¹æ³•äº†)ï¼Œé‚£éº¼ä½ éœ€è¦æ§‹é€ é…ç½®é¡ã€‚æ¯å€‹æ¨¡å‹éƒ½æœ‰å°æ‡‰çš„é…ç½®é¡ï¼Œæ¯”å¦‚DistilBertConfigã€‚ä½ å¯ä»¥é€šéå®ƒä¾†æŒ‡å®šéš±å–®å…ƒçš„å€‹æ•¸ï¼Œdropoutç­‰ç­‰ã€‚å¦‚æœä½ ä¿®æ”¹äº†æ ¸å¿ƒçš„è¶…åƒæ•¸(æ¯”å¦‚éš±å–®å…ƒçš„å€‹æ•¸)ï¼Œé‚£éº¼å°±ä¸èƒ½ä½¿ç”¨from_pretrainedåŠ è¼‰é è¨“ç·´çš„æ¨¡å‹äº†ï¼Œé€™æ™‚ä½ å¿…é ˆå¾é ­é–‹å§‹è¨“ç·´æ¨¡å‹ã€‚ç•¶ç„¶Tokenizerä¸€èˆ¬é‚„æ˜¯å¯ä»¥å¾©ç”¨çš„ã€‚

ä¸‹é¢çš„ä»£ç¢¼ä¿®æ”¹äº†æ ¸å¿ƒçš„è¶…åƒæ•¸ï¼Œæ§‹é€ äº†Tokenizerå’Œæ¨¡å‹å°è±¡ï¼š

```
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification(config)
```

å¦‚æœæˆ‘å€‘åªæ”¹è®Šæœ€å¾Œä¸€å±¤ï¼Œé€™æ˜¯å¾ˆå¸¸è¦‹çš„ï¼Œæ¯”å¦‚æŠŠä¸€å€‹å…©åˆ†é¡çš„æ¨¡å‹æ”¹æˆ10åˆ†é¡çš„æ¨¡å‹ï¼Œé‚£éº¼é‚„æ˜¯å¯ä»¥å¾©ç”¨ä¸‹é¢é‚£äº›å±¤çš„é è¨“ç·´æ¨¡å‹ã€‚æˆ‘å€‘å¯ä»¥ç²å–é è¨“ç·´æ¨¡å‹çš„bodyï¼Œç„¶å¾Œè‡ªå·±å¢åŠ ä¸€å€‹è¼¸å‡ºç‚º10çš„å…¨é€£æ¥å±¤ã€‚ä½†æ˜¯é€™è£¡æœ‰æ›´ç°¡å–®çš„æ–¹æ³•ï¼Œèª¿ç”¨from_pretrainedå‡½æ•¸ç„¶å¾Œè¨­ç½®num_labelsåƒæ•¸ï¼š

```
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
model_name = "distilbert-base-uncased"
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```

æˆ‘å€‘å¯ä»¥çœ‹ä¸€ä¸‹ä»£ç¢¼ï¼š

```
class DistilBertForSequenceClassification(DistilBertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.distilbert = DistilBertModel(config)
        self.pre_classifier = nn.Linear(config.dim, config.dim)
        self.classifier = nn.Linear(config.dim, config.num_labels)
        self.dropout = nn.Dropout(config.seq_classif_dropout)
    
        self.init_weights()
```

é è¨“ç·´æ¨¡å‹é€šå¸¸æ˜¯èªè¨€æ¨¡å‹(æ¯”å¦‚Masked LMæˆ–è€…NSPé€™æ¨£çš„ä»»å‹™)ï¼Œæ‰€ä»¥DistilBertForSequenceClassificationåªæœƒå¾©ç”¨å®ƒçš„bodyéƒ¨åˆ†ï¼Œheadéƒ¨åˆ†æ˜¯é‡æ–°æ§‹é€ çš„ã€‚



## Appendix

### Transformer Family Encoder æ¯”è¼ƒï¼šViT, CLIP, BLIP, BERT 

Ref: [@jizhiTransformerFamily2022]

å®Œæ•´çš„ transformer åŒ…å« encoder (discriminative) å’Œ decoder (generative).

ä»¥ä¸‹æˆ‘å€‘ä¸»è¦èšç„¦æ–¼ encoder éƒ¨åˆ†ã€‚å› çˆ² stable diffusion çš„ decoder (generative) éƒ¨åˆ†æ˜¯ç”± U-Net å®Œæˆçš„ã€‚é›–ç„¶ç›®å‰ä¹Ÿæœ‰ transformer-based çš„ decoder.  

<img src="/media/image-20230211213507007.png" alt="image-20230211213507007" style="zoom:33%;" />



|                    |   Type  |     Input    |      Output       |
| ------------------ | ------- | ------------ | ----------------- |
| Transformer | Encoder+decoder | Text        | Text              |
| BERT               | Encoder | Text         | Token Embeddings  |
| ViT                | Encoder | Image        | Token Embeddings  |
| CLIP               | Encoder | Text & Image | Similarity Score  |
| BLIP       | Encoder+decoder | Text & Image | Token Embeddings? |
| GPT1/2/3           | Decoder | Text         | Text              |







## Fine Tuning Network



## Appendix



#### BERT: Bidirectional Encoder Representations from Transformer

<img src="/media/image-20230211215131978.png" alt="image-20230211215131978" style="zoom:50%;" />

#### ViT:  Vision Transformer Encoder

<img src="/media/image-20230211213334026.png" alt="image-20230211213334026" style="zoom:50%;" />



#### CLIP: Contrastive **Language-Image** Pre-Training Encoder

CLIPæ˜¯ä¸€å€‹ **multi-modal vison and language model**ã€‚å®ƒå¯ç”¨æ–¼ (image-text) **åœ–åƒå’Œæ–‡æœ¬çš„ç›¸ä¼¼æ€§**ä»¥åŠ zero-**shot åœ–åƒåˆ†é¡ (è¦‹ä¸‹åœ–)**ã€‚CLIP ä½¿ç”¨ ViT-like transformer ç²å–**è¦–è¦ºç‰¹å¾µ**ï¼Œä¸¦ä½¿ç”¨å› æœèªè¨€æ¨¡å‹ (causal language model) ç²å–**æ–‡æœ¬ç‰¹å¾µ**ã€‚ç„¶å¾Œå°‡**æ–‡æœ¬å’Œè¦–è¦ºç‰¹å¾µæŠ•å½±åˆ°å…·æœ‰ç›¸åŒç¶­åº¦çš„ latent space**ã€‚æœ€å¾ŒæŠ•å½±åœ–åƒå’Œæ–‡æœ¬ç‰¹å¾µä¹‹é–“çš„å†…ç©ç”¢ç”Ÿç›¸ä¼¼æ€§çš„åˆ†æ•¸ (score of similarity)ã€‚

<img src="/media/image-20230211213942601.png" alt="image-20230211213942601" style="zoom: 50%;" />



#### BLIP: Bootstrapping Language-Image Pre-training Encoder/Decoder

<img src="/media/image-20230211214454365.png" alt="image-20230211214454365" style="zoom:50%;" />

<img src="/media/image-20230211214821598.png" alt="image-20230211214821598" style="zoom:50%;" />





